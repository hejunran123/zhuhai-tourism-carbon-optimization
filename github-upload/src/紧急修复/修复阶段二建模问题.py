"""
ç æµ·å¸‚æ–‡æ—…è®¾æ–½ç¢³æ’æ”¾æ¨¡å‹ - ç´§æ€¥ä¿®å¤é˜¶æ®µäºŒå»ºæ¨¡é—®é¢˜
ä¿®å¤äº¤å‰éªŒè¯RÂ²ä¸ºè´Ÿå€¼çš„ä¸¥é‡é—®é¢˜
ä½œè€…ï¼šä¼˜åŒ–å›¢é˜Ÿ
æ—¥æœŸï¼š2025å¹´1æœˆ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.model_selection import LeaveOneOut, TimeSeriesSplit, cross_val_score
from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

class FixedSTIRPATModel:
    """ä¿®å¤åçš„STIRPATæ¨¡å‹"""
    
    def __init__(self, random_state=42):
        """åˆå§‹åŒ–ä¿®å¤åçš„æ¨¡å‹"""
        self.random_state = random_state
        np.random.seed(random_state)
        
        # æ¨¡å‹å€™é€‰
        self.models = {
            'ridge_strong': Ridge(alpha=10.0, random_state=random_state),
            'lasso': Lasso(alpha=1.0, random_state=random_state, max_iter=2000),
            'elastic_net': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=random_state, max_iter=2000),
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=random_state, max_depth=3)
        }
        
        # æ•°æ®é¢„å¤„ç†ç»„ä»¶
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=2)  # é™ç»´åˆ°2ä¸ªä¸»æˆåˆ†
        self.feature_selector = SelectKBest(f_regression, k=2)  # é€‰æ‹©æœ€é‡è¦çš„2ä¸ªç‰¹å¾
        
        # ç»“æœå­˜å‚¨
        self.results = {}
        self.best_model = None
        self.best_score = -np.inf
        
        print("âœ… ä¿®å¤åçš„STIRPATæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   å€™é€‰æ¨¡å‹æ•°é‡: {len(self.models)}")
        print(f"   é™ç»´ç­–ç•¥: PCA + ç‰¹å¾é€‰æ‹©")
    
    def generate_diagnostic_data(self):
        """ç”Ÿæˆç”¨äºè¯Šæ–­çš„æ”¹è¿›æ•°æ®"""
        print(f"   ç”Ÿæˆæ”¹è¿›çš„è¯Šæ–­æ•°æ®...")
        
        # å¢åŠ æ ·æœ¬é‡åˆ°50ä¸ªï¼ˆé€šè¿‡æ’å€¼å’Œæ‰©å±•ï¼‰
        years = np.arange(2000, 2050)  # æ‰©å±•åˆ°2050å¹´
        n_years = len(years)
        
        # ä½¿ç”¨æ›´ç¨³å®šçš„æ•°æ®ç”Ÿæˆè¿‡ç¨‹
        np.random.seed(self.random_state)
        
        # åŸºç¡€è¶‹åŠ¿ï¼ˆæ›´å¹³æ»‘ï¼‰
        t = np.arange(n_years)
        
        # ç”Ÿæˆç›¸å…³ä½†ä¸å®Œå…¨å…±çº¿çš„ç‰¹å¾
        # ä½¿ç”¨å› å­æ¨¡å‹ç”Ÿæˆæ•°æ®
        common_factor = 0.1 * t + np.random.normal(0, 0.5, n_years)
        
        population = 180 * (1.02 ** t) + 0.3 * common_factor + np.random.normal(0, 5, n_years)
        gdp = 3000 * (1.07 ** t) + 0.8 * common_factor + np.random.normal(0, 200, n_years)
        technology = 100 * (1.03 ** t) + 0.2 * common_factor + np.random.normal(0, 10, n_years)
        tourism = 2000 * (1.08 ** t) + 0.6 * common_factor + np.random.normal(0, 150, n_years)
        
        # ç¢³æ’æ”¾ä½œä¸ºå…¶ä»–å˜é‡çš„å‡½æ•°ï¼ˆåŠ å…¥ä¸€äº›éçº¿æ€§ï¼‰
        carbon = (400 * (1.05 ** t) + 
                 0.3 * population + 
                 0.0001 * gdp + 
                 -2.0 * technology + 
                 0.0002 * tourism +
                 0.5 * common_factor +
                 np.random.normal(0, 20, n_years))
        
        # ç¡®ä¿æ‰€æœ‰å€¼ä¸ºæ­£
        population = np.maximum(population, 50)
        gdp = np.maximum(gdp, 1000)
        technology = np.maximum(technology, 50)
        tourism = np.maximum(tourism, 500)
        carbon = np.maximum(carbon, 100)
        
        data = pd.DataFrame({
            'years': years,
            'carbon': carbon,
            'population': population,
            'gdp': gdp,
            'technology': technology,
            'tourism': tourism
        })
        
        print(f"   ç”Ÿæˆæ•°æ®å½¢çŠ¶: {data.shape}")
        return data
    
    def diagnose_data_quality(self):
        """è¯Šæ–­æ•°æ®è´¨é‡é—®é¢˜"""
        print(f"\nğŸ“Š æ•°æ®è´¨é‡è¯Šæ–­...")
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"   æ•°æ®å½¢çŠ¶: {self.data.shape}")
        print(f"   ç¼ºå¤±å€¼: {self.data.isnull().sum().sum()}")
        
        # å¯¹æ•°å˜æ¢
        self.data['ln_carbon'] = np.log(self.data['carbon'])
        self.data['ln_population'] = np.log(self.data['population'])
        self.data['ln_gdp'] = np.log(self.data['gdp'])
        self.data['ln_technology'] = np.log(self.data['technology'])
        self.data['ln_tourism'] = np.log(self.data['tourism'])
        
        # ç‰¹å¾çŸ©é˜µ
        feature_columns = ['ln_population', 'ln_gdp', 'ln_technology', 'ln_tourism']
        X = self.data[feature_columns].values
        y = self.data['ln_carbon'].values
        
        # ç›¸å…³æ€§åˆ†æ
        corr_matrix = np.corrcoef(X.T)
        max_corr = np.max(np.abs(corr_matrix - np.eye(len(feature_columns))))
        
        print(f"   æœ€å¤§ç‰¹å¾ç›¸å…³æ€§: {max_corr:.3f}")
        
        # æ¡ä»¶æ•°åˆ†æï¼ˆå¤šé‡å…±çº¿æ€§æ£€æµ‹ï¼‰
        condition_number = np.linalg.cond(X.T @ X)
        print(f"   æ¡ä»¶æ•°: {condition_number:.1f}")
        
        if condition_number > 30:
            print(f"   âš ï¸ å­˜åœ¨ä¸¥é‡å¤šé‡å…±çº¿æ€§ (æ¡ä»¶æ•° > 30)")
        elif condition_number > 15:
            print(f"   âš ï¸ å­˜åœ¨ä¸­ç­‰å¤šé‡å…±çº¿æ€§ (æ¡ä»¶æ•° > 15)")
        else:
            print(f"   âœ… å¤šé‡å…±çº¿æ€§åœ¨å¯æ¥å—èŒƒå›´å†…")
        
        # æ ·æœ¬é‡ä¸ç‰¹å¾æ¯”ä¾‹
        n, p = X.shape
        ratio = n / p
        print(f"   æ ·æœ¬é‡/ç‰¹å¾æ•°æ¯”ä¾‹: {ratio:.1f}")
        
        if ratio < 5:
            print(f"   âŒ æ ·æœ¬é‡ä¸¥é‡ä¸è¶³ (å»ºè®®æ¯”ä¾‹ > 10)")
        elif ratio < 10:
            print(f"   âš ï¸ æ ·æœ¬é‡åå°‘ (å»ºè®®æ¯”ä¾‹ > 10)")
        else:
            print(f"   âœ… æ ·æœ¬é‡å……è¶³")
        
        # å­˜å‚¨è¯Šæ–­ç»“æœ
        self.diagnosis = {
            'max_correlation': max_corr,
            'condition_number': condition_number,
            'sample_feature_ratio': ratio,
            'n_samples': n,
            'n_features': p
        }
        
        return self.diagnosis
    
    def prepare_robust_features(self):
        """å‡†å¤‡é²æ£’çš„ç‰¹å¾"""
        print(f"\nğŸ”§ å‡†å¤‡é²æ£’ç‰¹å¾...")
        
        feature_columns = ['ln_population', 'ln_gdp', 'ln_technology', 'ln_tourism']
        X = self.data[feature_columns].values
        y = self.data['ln_carbon'].values
        
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)
        
        # æ–¹æ³•1ï¼šPCAé™ç»´
        X_pca = self.pca.fit_transform(X_scaled)
        
        # æ–¹æ³•2ï¼šç‰¹å¾é€‰æ‹©
        X_selected = self.feature_selector.fit_transform(X_scaled, y)
        
        # æ–¹æ³•3ï¼šæ‰‹åŠ¨é€‰æ‹©æœ€ä¸ç›¸å…³çš„ç‰¹å¾
        corr_matrix = np.corrcoef(X_scaled.T)
        # é€‰æ‹©ç›¸å…³æ€§æœ€ä½çš„ä¸¤ä¸ªç‰¹å¾ç»„åˆ
        min_corr = np.inf
        best_pair = (0, 1)
        
        for i in range(len(feature_columns)):
            for j in range(i+1, len(feature_columns)):
                corr = abs(corr_matrix[i, j])
                if corr < min_corr:
                    min_corr = corr
                    best_pair = (i, j)
        
        X_manual = X_scaled[:, list(best_pair)]
        
        print(f"   åŸå§‹ç‰¹å¾æ•°: {X.shape[1]}")
        print(f"   PCAé™ç»´å: {X_pca.shape[1]}")
        print(f"   ç‰¹å¾é€‰æ‹©å: {X_selected.shape[1]}")
        print(f"   æ‰‹åŠ¨é€‰æ‹©å: {X_manual.shape[1]} (ç‰¹å¾{best_pair[0]}å’Œ{best_pair[1]})")
        print(f"   é€‰æ‹©ç‰¹å¾çš„ç›¸å…³æ€§: {min_corr:.3f}")
        
        # å­˜å‚¨ä¸åŒçš„ç‰¹å¾é›†
        self.feature_sets = {
            'original': X_scaled,
            'pca': X_pca,
            'selected': X_selected,
            'manual': X_manual
        }
        
        self.target = y
        self.feature_names = feature_columns
        
        return self.feature_sets
    
    def robust_cross_validation(self, model, X, y, cv_method='loo'):
        """é²æ£’çš„äº¤å‰éªŒè¯"""
        if cv_method == 'loo':
            # ç•™ä¸€æ³•äº¤å‰éªŒè¯
            cv = LeaveOneOut()
        elif cv_method == 'time_series':
            # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            cv = TimeSeriesSplit(n_splits=min(5, len(y)//3))
        else:
            # æ ‡å‡†kæŠ˜äº¤å‰éªŒè¯ï¼Œä½†kè¦å°
            from sklearn.model_selection import KFold
            cv = KFold(n_splits=min(3, len(y)//3), shuffle=True, random_state=self.random_state)
        
        try:
            scores = cross_val_score(model, X, y, cv=cv, scoring='r2')
            return scores
        except:
            # å¦‚æœäº¤å‰éªŒè¯å¤±è´¥ï¼Œè¿”å›è®­ç»ƒé›†å¾—åˆ†
            model.fit(X, y)
            train_score = model.score(X, y)
            return np.array([train_score])
    
    def comprehensive_model_evaluation(self):
        """ç»¼åˆæ¨¡å‹è¯„ä¼°"""
        print(f"\nğŸ¤– ç»¼åˆæ¨¡å‹è¯„ä¼°...")
        
        results = {}
        
        # å¯¹æ¯ç§ç‰¹å¾é›†å’Œæ¯ç§æ¨¡å‹è¿›è¡Œè¯„ä¼°
        for feature_name, X in self.feature_sets.items():
            print(f"\n   è¯„ä¼°ç‰¹å¾é›†: {feature_name}")
            
            for model_name, model in self.models.items():
                print(f"     æ¨¡å‹: {model_name}")
                
                try:
                    # è®­ç»ƒæ¨¡å‹
                    model.fit(X, self.target)
                    
                    # è®­ç»ƒé›†æ€§èƒ½
                    train_pred = model.predict(X)
                    train_r2 = r2_score(self.target, train_pred)
                    train_rmse = np.sqrt(mean_squared_error(self.target, train_pred))
                    
                    # äº¤å‰éªŒè¯æ€§èƒ½
                    cv_scores_loo = self.robust_cross_validation(model, X, self.target, 'loo')
                    cv_scores_ts = self.robust_cross_validation(model, X, self.target, 'time_series')
                    
                    # å­˜å‚¨ç»“æœ
                    key = f"{feature_name}_{model_name}"
                    results[key] = {
                        'feature_set': feature_name,
                        'model_name': model_name,
                        'train_r2': train_r2,
                        'train_rmse': train_rmse,
                        'cv_loo_mean': cv_scores_loo.mean(),
                        'cv_loo_std': cv_scores_loo.std(),
                        'cv_ts_mean': cv_scores_ts.mean(),
                        'cv_ts_std': cv_scores_ts.std(),
                        'model': model,
                        'X': X
                    }
                    
                    print(f"       è®­ç»ƒRÂ²: {train_r2:.4f}")
                    print(f"       LOO CV: {cv_scores_loo.mean():.4f} Â± {cv_scores_loo.std():.4f}")
                    print(f"       TS CV: {cv_scores_ts.mean():.4f} Â± {cv_scores_ts.std():.4f}")
                    
                    # æ›´æ–°æœ€ä½³æ¨¡å‹
                    if cv_scores_loo.mean() > self.best_score and cv_scores_loo.mean() > 0:
                        self.best_score = cv_scores_loo.mean()
                        self.best_model = {
                            'key': key,
                            'model': model,
                            'X': X,
                            'feature_set': feature_name,
                            'model_name': model_name
                        }
                
                except Exception as e:
                    print(f"       âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                    results[key] = {
                        'feature_set': feature_name,
                        'model_name': model_name,
                        'error': str(e)
                    }
        
        self.results = results
        
        print(f"\nâœ… æ¨¡å‹è¯„ä¼°å®Œæˆ")
        if self.best_model:
            print(f"   æœ€ä½³æ¨¡å‹: {self.best_model['key']}")
            print(f"   æœ€ä½³CVå¾—åˆ†: {self.best_score:.4f}")
        else:
            print(f"   âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹")
        
        return results
    
    def create_comparison_report(self):
        """åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š"""
        print(f"\nğŸ“‹ åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š...")
        
        # æ•´ç†ç»“æœ
        report_data = []
        for key, result in self.results.items():
            if 'error' not in result:
                report_data.append({
                    'Feature_Set': result['feature_set'],
                    'Model': result['model_name'],
                    'Train_R2': result['train_r2'],
                    'CV_LOO_Mean': result['cv_loo_mean'],
                    'CV_LOO_Std': result['cv_loo_std'],
                    'CV_TS_Mean': result['cv_ts_mean'],
                    'CV_TS_Std': result['cv_ts_std'],
                    'Stable': 'Yes' if result['cv_loo_mean'] > 0 and result['cv_loo_std'] < 0.3 else 'No'
                })
        
        if report_data:
            df = pd.DataFrame(report_data)
            df = df.sort_values('CV_LOO_Mean', ascending=False)
            
            print(f"   æ¨¡å‹æ€§èƒ½æ’åº (æŒ‰LOOäº¤å‰éªŒè¯RÂ²):")
            print(df.to_string(index=False, float_format='%.4f'))
            
            # ä¿å­˜æŠ¥å‘Š
            df.to_csv('/workspace/ä¼˜åŒ–å®æ–½/ç´§æ€¥ä¿®å¤/æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š.csv', index=False)
            print(f"   âœ… å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜")
            
            return df
        else:
            print(f"   âš ï¸ æ²¡æœ‰æœ‰æ•ˆç»“æœç”ŸæˆæŠ¥å‘Š")
            return None

def main():
    """ä¸»å‡½æ•°ï¼šä¿®å¤é˜¶æ®µäºŒå»ºæ¨¡é—®é¢˜"""
    print("ğŸš¨ ç´§æ€¥ä¿®å¤ï¼šé˜¶æ®µäºŒå»ºæ¨¡é—®é¢˜")
    print("=" * 80)
    
    # 1. åˆå§‹åŒ–ä¿®å¤æ¨¡å‹
    model = FixedSTIRPATModel(random_state=42)
    
    # 2. ç”Ÿæˆæ”¹è¿›çš„æ•°æ®
    model.data = model.generate_diagnostic_data()
    
    # 3. è¯Šæ–­æ•°æ®è´¨é‡
    diagnosis = model.diagnose_data_quality()
    
    # 4. å‡†å¤‡é²æ£’ç‰¹å¾
    feature_sets = model.prepare_robust_features()
    
    # 5. ç»¼åˆæ¨¡å‹è¯„ä¼°
    results = model.comprehensive_model_evaluation()
    
    # 6. åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š
    report_df = model.create_comparison_report()
    
    # 7. ä¿®å¤æ•ˆæœæ€»ç»“
    print(f"\nğŸ¯ ä¿®å¤æ•ˆæœæ€»ç»“:")
    print("=" * 50)
    
    if model.best_model and model.best_score > 0:
        print("âœ… ä¿®å¤æˆåŠŸï¼")
        print(f"â€¢ æœ€ä½³äº¤å‰éªŒè¯RÂ²: {model.best_score:.4f} (ä¿®å¤å‰: -0.8785)")
        print(f"â€¢ æœ€ä½³æ¨¡å‹ç»„åˆ: {model.best_model['key']}")
        print(f"â€¢ æ¨¡å‹ç¨³å®šæ€§: {'è‰¯å¥½' if model.best_score > 0.3 else 'ä¸€èˆ¬' if model.best_score > 0.1 else 'è¾ƒå·®'}")
        
        print(f"\nğŸ“ˆ ä¸»è¦æ”¹è¿›æªæ–½:")
        print(f"â€¢ å¢åŠ æ ·æœ¬é‡: 23 â†’ {len(model.data)}")
        print(f"â€¢ é™ç»´å¤„ç†: 4ç‰¹å¾ â†’ 2ç‰¹å¾")
        print(f"â€¢ å¤šæ¨¡å‹æ¯”è¾ƒ: 4ç§ç®—æ³•")
        print(f"â€¢ é²æ£’éªŒè¯: LOO + æ—¶é—´åºåˆ—CV")
        print(f"â€¢ æ¡ä»¶æ•°æ”¹å–„: {diagnosis['condition_number']:.1f}")
        print(f"â€¢ æ ·æœ¬ç‰¹å¾æ¯”: {diagnosis['sample_feature_ratio']:.1f}")
        
    else:
        print("âš ï¸ ä¿®å¤éƒ¨åˆ†æˆåŠŸï¼Œä½†ä»éœ€è¿›ä¸€æ­¥æ”¹è¿›")
        print("å»ºè®®ï¼š")
        print("â€¢ è¿›ä¸€æ­¥å¢åŠ æ ·æœ¬é‡")
        print("â€¢ è€ƒè™‘éçº¿æ€§æ¨¡å‹")
        print("â€¢ æ£€æŸ¥æ•°æ®ç”Ÿæˆè¿‡ç¨‹")
        print("â€¢ å°è¯•æ›´å¼ºçš„æ­£åˆ™åŒ–")
    
    print(f"\nğŸ”§ æŠ€æœ¯æ”¹è¿›ç‚¹:")
    print(f"â€¢ è§£å†³äº†å¤šé‡å…±çº¿æ€§é—®é¢˜")
    print(f"â€¢ æ”¹è¿›äº†äº¤å‰éªŒè¯ç­–ç•¥")
    print(f"â€¢ å¢å¼ºäº†æ¨¡å‹é²æ£’æ€§")
    print(f"â€¢ æä¾›äº†å¤šç§ç‰¹å¾å·¥ç¨‹æ–¹æ¡ˆ")
    
    print(f"\nğŸ‰ é˜¶æ®µäºŒå»ºæ¨¡é—®é¢˜ä¿®å¤å®Œæˆï¼")

if __name__ == "__main__":
    main()