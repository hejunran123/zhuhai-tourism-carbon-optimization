"""
ç æµ·å¸‚æ–‡æ—…è®¾æ–½ç¢³æ’æ”¾æ¨¡å‹ - è´å¶æ–¯STIRPATå»ºæ¨¡ç³»ç»Ÿ
é˜¶æ®µäºŒï¼šå»ºæ¨¡æ–¹æ³•ä¼˜åŒ–
ä½œè€…ï¼šä¼˜åŒ–å›¢é˜Ÿ
æ—¥æœŸï¼š2025å¹´1æœˆ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.linear_model import Ridge, BayesianRidge
from sklearn.metrics import r2_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

class BayesianSTIRPATModel:
    """è´å¶æ–¯STIRPATæ¨¡å‹"""
    
    def __init__(self, random_state=42):
        """åˆå§‹åŒ–è´å¶æ–¯STIRPATæ¨¡å‹"""
        self.random_state = random_state
        np.random.seed(random_state)
        
        # æ¨¡å‹ç»„ä»¶
        self.scaler = StandardScaler()
        self.bayesian_model = BayesianRidge(
            alpha_1=1e-6, alpha_2=1e-6,  # ç²¾åº¦å‚æ•°çš„å…ˆéªŒ
            lambda_1=1e-6, lambda_2=1e-6,  # æƒé‡å‚æ•°çš„å…ˆéªŒ
            compute_score=True,
            fit_intercept=True
        )
        
        # å…ˆéªŒçŸ¥è¯†ï¼ˆåŸºäºæ–‡çŒ®ç ”ç©¶ï¼‰
        self.prior_knowledge = {
            'population_elasticity': {'mean': 0.3, 'std': 0.2},
            'gdp_elasticity': {'mean': 0.8, 'std': 0.3},
            'technology_elasticity': {'mean': -0.5, 'std': 0.2},
            'tourism_elasticity': {'mean': 0.4, 'std': 0.2}
        }
        
        # å­˜å‚¨ç»“æœ
        self.model_results = {}
        self.uncertainty_results = {}
        
        print("âœ… è´å¶æ–¯STIRPATæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   éšæœºç§å­: {random_state}")
        print(f"   å…ˆéªŒçŸ¥è¯†: {len(self.prior_knowledge)}ä¸ªå‚æ•°")
    
    def load_data(self, data_path=None):
        """åŠ è½½æ—¶é—´åºåˆ—æ•°æ®"""
        if data_path is None:
            data_path = '/workspace/ä¼˜åŒ–å®æ–½/é˜¶æ®µä¸€_æ•°æ®è´¨é‡ä¼˜åŒ–/ä¼˜åŒ–åæ—¶é—´åºåˆ—æ•°æ®.csv'
        
        print(f"\nğŸ“Š åŠ è½½æ—¶é—´åºåˆ—æ•°æ®...")
        
        try:
            self.data = pd.read_csv(data_path)
            print(f"   æ•°æ®æ–‡ä»¶: {data_path}")
            print(f"   æ•°æ®å½¢çŠ¶: {self.data.shape}")
            print(f"   æ—¶é—´è·¨åº¦: {self.data['years'].min()} - {self.data['years'].max()}")
            
            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            missing_data = self.data.isnull().sum()
            if missing_data.sum() > 0:
                print(f"   ç¼ºå¤±æ•°æ®: {missing_data.sum()}ä¸ª")
            else:
                print(f"   æ•°æ®å®Œæ•´æ€§: 100%")
            
            return self.data
            
        except FileNotFoundError:
            print(f"âš ï¸ æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
            return self.generate_sample_data()
    
    def generate_sample_data(self):
        """ç”Ÿæˆç¤ºä¾‹æ•°æ®"""
        years = np.arange(2000, 2023)
        n_years = len(years)
        
        # åŸºäºåˆç†çš„ç»æµå¢é•¿æ¨¡å¼ç”Ÿæˆæ•°æ®
        base_carbon = 400
        base_pop = 180
        base_gdp = 3000
        base_tech = 100
        base_tourism = 2000
        
        data = {
            'years': years,
            'carbon': base_carbon * (1.08 ** np.arange(n_years)) * (1 + np.random.normal(0, 0.05, n_years)),
            'population': base_pop * (1.025 ** np.arange(n_years)) * (1 + np.random.normal(0, 0.02, n_years)),
            'gdp': base_gdp * (1.09 ** np.arange(n_years)) * (1 + np.random.normal(0, 0.08, n_years)),
            'technology': base_tech * (1.04 ** np.arange(n_years)) * (1 + np.random.normal(0, 0.03, n_years)),
            'tourism': base_tourism * (1.12 ** np.arange(n_years)) * (1 + np.random.normal(0, 0.10, n_years))
        }
        
        self.data = pd.DataFrame(data)
        print(f"   ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®: {self.data.shape}")
        
        return self.data
    
    def prepare_stirpat_data(self):
        """å‡†å¤‡STIRPATå»ºæ¨¡æ•°æ®"""
        print(f"\nğŸ”§ å‡†å¤‡STIRPATå»ºæ¨¡æ•°æ®...")
        
        # å¯¹æ•°å˜æ¢
        self.data['ln_carbon'] = np.log(self.data['carbon'])
        self.data['ln_population'] = np.log(self.data['population'])
        self.data['ln_gdp'] = np.log(self.data['gdp'])
        self.data['ln_technology'] = np.log(self.data['technology'])
        self.data['ln_tourism'] = np.log(self.data['tourism'])
        
        # æ„å»ºç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å˜é‡
        feature_columns = ['ln_population', 'ln_gdp', 'ln_technology', 'ln_tourism']
        self.X = self.data[feature_columns].values
        self.y = self.data['ln_carbon'].values
        self.feature_names = ['äººå£', 'GDP', 'æŠ€æœ¯', 'æ—…æ¸¸']
        
        # æ•°æ®æ ‡å‡†åŒ–
        self.X_scaled = self.scaler.fit_transform(self.X)
        
        print(f"   ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {self.X.shape}")
        print(f"   ç›®æ ‡å˜é‡å½¢çŠ¶: {self.y.shape}")
        print(f"   ç‰¹å¾åç§°: {self.feature_names}")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        self.check_data_quality()
        
        return self.X_scaled, self.y
    
    def check_data_quality(self):
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        print(f"\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥...")
        
        # æ£€æŸ¥å¤šé‡å…±çº¿æ€§
        correlation_matrix = np.corrcoef(self.X.T)
        max_corr = np.max(np.abs(correlation_matrix - np.eye(len(self.feature_names))))
        
        print(f"   æœ€å¤§ç‰¹å¾ç›¸å…³æ€§: {max_corr:.3f}")
        if max_corr > 0.8:
            print(f"   âš ï¸ å­˜åœ¨é«˜åº¦ç›¸å…³çš„ç‰¹å¾ï¼Œå¯èƒ½å½±å“æ¨¡å‹ç¨³å®šæ€§")
        else:
            print(f"   âœ… ç‰¹å¾ç›¸å…³æ€§åœ¨åˆç†èŒƒå›´å†…")
        
        # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
        for i, name in enumerate(self.feature_names):
            skewness = stats.skew(self.X[:, i])
            print(f"   {name}ååº¦: {skewness:.3f}")
        
        target_skewness = stats.skew(self.y)
        print(f"   ç›®æ ‡å˜é‡ååº¦: {target_skewness:.3f}")
    
    def fit_bayesian_model(self):
        """æ‹Ÿåˆè´å¶æ–¯æ¨¡å‹"""
        print(f"\nğŸ¤– æ‹Ÿåˆè´å¶æ–¯STIRPATæ¨¡å‹...")
        
        # æ‹Ÿåˆè´å¶æ–¯å²­å›å½’
        self.bayesian_model.fit(self.X_scaled, self.y)
        
        # é¢„æµ‹
        y_pred = self.bayesian_model.predict(self.X_scaled)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        r2 = r2_score(self.y, y_pred)
        rmse = np.sqrt(mean_squared_error(self.y, y_pred))
        
        # è·å–æ¨¡å‹å‚æ•°
        coefficients = self.bayesian_model.coef_
        intercept = self.bayesian_model.intercept_
        
        # è®¡ç®—å‚æ•°çš„ä¸ç¡®å®šæ€§ï¼ˆåŸºäºè´å¶æ–¯æ¨æ–­ï¼‰
        # ä½¿ç”¨æ¨¡å‹çš„åæ–¹å·®çŸ©é˜µä¼°è®¡å‚æ•°ä¸ç¡®å®šæ€§
        sigma_squared = self.bayesian_model.sigma_
        coef_std = np.sqrt(np.diag(sigma_squared))
        
        self.model_results = {
            'coefficients': coefficients,
            'intercept': intercept,
            'coef_std': coef_std,
            'r2_score': r2,
            'rmse': rmse,
            'predictions': y_pred,
            'alpha': self.bayesian_model.alpha_,
            'lambda': self.bayesian_model.lambda_
        }
        
        print(f"âœ… è´å¶æ–¯æ¨¡å‹æ‹Ÿåˆå®Œæˆ")
        print(f"   RÂ²: {r2:.4f}")
        print(f"   RMSE: {rmse:.4f}")
        print(f"   ç²¾åº¦å‚æ•°Î±: {self.bayesian_model.alpha_:.6f}")
        print(f"   æ­£åˆ™åŒ–å‚æ•°Î»: {self.bayesian_model.lambda_:.6f}")
        
        # æ‰“å°ç³»æ•°ç»“æœ
        print(f"\nğŸ“Š æ¨¡å‹ç³»æ•°ç»“æœ:")
        for i, (name, coef, std) in enumerate(zip(self.feature_names, coefficients, coef_std)):
            print(f"   {name}: {coef:.4f} Â± {std:.4f}")
        
        return self.model_results
    
    def uncertainty_quantification(self, n_samples=1000):
        """ä¸ç¡®å®šæ€§é‡åŒ–"""
        print(f"\nğŸ¯ è¿›è¡Œä¸ç¡®å®šæ€§é‡åŒ–...")
        
        # ä½¿ç”¨è´å¶æ–¯æ¨æ–­ç”Ÿæˆå‚æ•°çš„åéªŒåˆ†å¸ƒæ ·æœ¬
        coef_samples = []
        
        for i in range(n_samples):
            # ä»åéªŒåˆ†å¸ƒä¸­é‡‡æ ·å‚æ•°
            noise = np.random.normal(0, self.model_results['coef_std'])
            coef_sample = self.model_results['coefficients'] + noise
            coef_samples.append(coef_sample)
        
        coef_samples = np.array(coef_samples)
        
        # è®¡ç®—é¢„æµ‹çš„ä¸ç¡®å®šæ€§
        pred_samples = []
        for coef_sample in coef_samples:
            pred = self.X_scaled @ coef_sample + self.model_results['intercept']
            pred_samples.append(pred)
        
        pred_samples = np.array(pred_samples)
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        pred_mean = np.mean(pred_samples, axis=0)
        pred_std = np.std(pred_samples, axis=0)
        pred_lower = np.percentile(pred_samples, 2.5, axis=0)
        pred_upper = np.percentile(pred_samples, 97.5, axis=0)
        
        self.uncertainty_results = {
            'coef_samples': coef_samples,
            'pred_samples': pred_samples,
            'pred_mean': pred_mean,
            'pred_std': pred_std,
            'pred_lower': pred_lower,
            'pred_upper': pred_upper
        }
        
        print(f"âœ… ä¸ç¡®å®šæ€§é‡åŒ–å®Œæˆ")
        print(f"   å‚æ•°æ ·æœ¬æ•°: {n_samples}")
        print(f"   é¢„æµ‹ä¸ç¡®å®šæ€§èŒƒå›´: {pred_std.mean():.4f} Â± {pred_std.std():.4f}")
        
        return self.uncertainty_results
    
    def cross_validation(self, cv_folds=5):
        """äº¤å‰éªŒè¯"""
        print(f"\nğŸ”„ è¿›è¡Œ{cv_folds}æŠ˜äº¤å‰éªŒè¯...")
        
        # æ‰§è¡Œäº¤å‰éªŒè¯
        cv_scores = cross_val_score(
            self.bayesian_model, self.X_scaled, self.y, 
            cv=cv_folds, scoring='r2'
        )
        
        cv_mean = cv_scores.mean()
        cv_std = cv_scores.std()
        
        print(f"âœ… äº¤å‰éªŒè¯å®Œæˆ")
        print(f"   å¹³å‡RÂ²: {cv_mean:.4f}")
        print(f"   æ ‡å‡†å·®: {cv_std:.4f}")
        print(f"   95%ç½®ä¿¡åŒºé—´: [{cv_mean - 1.96*cv_std:.4f}, {cv_mean + 1.96*cv_std:.4f}]")
        
        # è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§
        if cv_std < 0.1:
            print(f"   âœ… æ¨¡å‹ç¨³å®šæ€§: è‰¯å¥½")
        elif cv_std < 0.2:
            print(f"   âš ï¸ æ¨¡å‹ç¨³å®šæ€§: ä¸­ç­‰")
        else:
            print(f"   âŒ æ¨¡å‹ç¨³å®šæ€§: è¾ƒå·®")
        
        return cv_scores
    
    def compare_with_traditional_methods(self):
        """ä¸ä¼ ç»Ÿæ–¹æ³•æ¯”è¾ƒ"""
        print(f"\nğŸ“ˆ ä¸ä¼ ç»Ÿæ–¹æ³•æ¯”è¾ƒ...")
        
        # ä¼ ç»Ÿå²­å›å½’
        ridge_model = Ridge(alpha=1.0, random_state=self.random_state)
        ridge_model.fit(self.X_scaled, self.y)
        ridge_pred = ridge_model.predict(self.X_scaled)
        ridge_r2 = r2_score(self.y, ridge_pred)
        ridge_rmse = np.sqrt(mean_squared_error(self.y, ridge_pred))
        
        # æ™®é€šæœ€å°äºŒä¹˜ï¼ˆæ— æ­£åˆ™åŒ–ï¼‰
        from sklearn.linear_model import LinearRegression
        ols_model = LinearRegression()
        ols_model.fit(self.X_scaled, self.y)
        ols_pred = ols_model.predict(self.X_scaled)
        ols_r2 = r2_score(self.y, ols_pred)
        ols_rmse = np.sqrt(mean_squared_error(self.y, ols_pred))
        
        # æ¯”è¾ƒç»“æœ
        comparison_results = {
            'Bayesian Ridge': {
                'RÂ²': self.model_results['r2_score'],
                'RMSE': self.model_results['rmse'],
                'Uncertainty': 'Yes'
            },
            'Ridge Regression': {
                'RÂ²': ridge_r2,
                'RMSE': ridge_rmse,
                'Uncertainty': 'No'
            },
            'OLS': {
                'RÂ²': ols_r2,
                'RMSE': ols_rmse,
                'Uncertainty': 'No'
            }
        }
        
        print(f"   æ–¹æ³•æ¯”è¾ƒç»“æœ:")
        for method, metrics in comparison_results.items():
            print(f"   {method}:")
            print(f"     RÂ²: {metrics['RÂ²']:.4f}")
            print(f"     RMSE: {metrics['RMSE']:.4f}")
            print(f"     ä¸ç¡®å®šæ€§é‡åŒ–: {metrics['Uncertainty']}")
        
        return comparison_results
    
    def scenario_prediction(self, scenarios):
        """æƒ…æ™¯é¢„æµ‹"""
        print(f"\nğŸ”® è¿›è¡Œæƒ…æ™¯é¢„æµ‹...")
        
        predictions = {}
        
        for scenario_name, scenario_data in scenarios.items():
            print(f"   é¢„æµ‹æƒ…æ™¯: {scenario_name}")
            
            # å‡†å¤‡é¢„æµ‹æ•°æ®
            X_scenario = np.array([[
                np.log(scenario_data['population']),
                np.log(scenario_data['gdp']),
                np.log(scenario_data['technology']),
                np.log(scenario_data['tourism'])
            ]])
            
            X_scenario_scaled = self.scaler.transform(X_scenario)
            
            # ç‚¹é¢„æµ‹
            pred_mean = self.bayesian_model.predict(X_scenario_scaled)[0]
            
            # ä¸ç¡®å®šæ€§é¢„æµ‹
            if hasattr(self, 'uncertainty_results'):
                pred_samples = []
                for coef_sample in self.uncertainty_results['coef_samples'][:100]:  # ä½¿ç”¨100ä¸ªæ ·æœ¬
                    pred_sample = X_scenario_scaled @ coef_sample + self.model_results['intercept']
                    pred_samples.append(pred_sample[0])
                
                pred_samples = np.array(pred_samples)
                pred_std = np.std(pred_samples)
                pred_lower = np.percentile(pred_samples, 2.5)
                pred_upper = np.percentile(pred_samples, 97.5)
            else:
                pred_std = pred_lower = pred_upper = None
            
            # è½¬æ¢å›åŸå§‹å°ºåº¦
            carbon_pred = np.exp(pred_mean)
            carbon_lower = np.exp(pred_lower) if pred_lower is not None else None
            carbon_upper = np.exp(pred_upper) if pred_upper is not None else None
            
            predictions[scenario_name] = {
                'ln_carbon_pred': pred_mean,
                'carbon_pred': carbon_pred,
                'carbon_lower': carbon_lower,
                'carbon_upper': carbon_upper,
                'uncertainty_std': pred_std
            }
            
            print(f"     é¢„æµ‹ç¢³æ’æ”¾: {carbon_pred:.1f} ä¸‡å¨CO2")
            if carbon_lower is not None:
                print(f"     95%ç½®ä¿¡åŒºé—´: [{carbon_lower:.1f}, {carbon_upper:.1f}] ä¸‡å¨CO2")
        
        return predictions
    
    def visualize_results(self, save_path=None):
        """å¯è§†åŒ–ç»“æœ"""
        print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. å®é™…å€¼vsé¢„æµ‹å€¼
        axes[0, 0].scatter(self.y, self.model_results['predictions'], alpha=0.7)
        axes[0, 0].plot([self.y.min(), self.y.max()], [self.y.min(), self.y.max()], 'r--', lw=2)
        axes[0, 0].set_xlabel('å®é™…å€¼ (ln_carbon)')
        axes[0, 0].set_ylabel('é¢„æµ‹å€¼ (ln_carbon)')
        axes[0, 0].set_title(f'å®é™…å€¼ vs é¢„æµ‹å€¼ (RÂ² = {self.model_results["r2_score"]:.3f})')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. æ®‹å·®å›¾
        residuals = self.y - self.model_results['predictions']
        axes[0, 1].scatter(self.model_results['predictions'], residuals, alpha=0.7)
        axes[0, 1].axhline(y=0, color='r', linestyle='--')
        axes[0, 1].set_xlabel('é¢„æµ‹å€¼')
        axes[0, 1].set_ylabel('æ®‹å·®')
        axes[0, 1].set_title('æ®‹å·®åˆ†å¸ƒ')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ç³»æ•°åŠå…¶ä¸ç¡®å®šæ€§
        coefficients = self.model_results['coefficients']
        coef_std = self.model_results['coef_std']
        
        x_pos = np.arange(len(self.feature_names))
        axes[0, 2].bar(x_pos, coefficients, yerr=coef_std, capsize=5, alpha=0.7)
        axes[0, 2].set_xlabel('ç‰¹å¾')
        axes[0, 2].set_ylabel('ç³»æ•°å€¼')
        axes[0, 2].set_title('æ¨¡å‹ç³»æ•°åŠä¸ç¡®å®šæ€§')
        axes[0, 2].set_xticks(x_pos)
        axes[0, 2].set_xticklabels(self.feature_names, rotation=45)
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. æ—¶é—´åºåˆ—æ‹Ÿåˆ
        years = self.data['years']
        actual_carbon = self.data['carbon']
        predicted_carbon = np.exp(self.model_results['predictions'])
        
        axes[1, 0].plot(years, actual_carbon, 'b-', linewidth=2, label='å®é™…å€¼')
        axes[1, 0].plot(years, predicted_carbon, 'r--', linewidth=2, label='é¢„æµ‹å€¼')
        axes[1, 0].set_xlabel('å¹´ä»½')
        axes[1, 0].set_ylabel('ç¢³æ’æ”¾é‡ (ä¸‡å¨CO2)')
        axes[1, 0].set_title('æ—¶é—´åºåˆ—æ‹Ÿåˆ')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. é¢„æµ‹ä¸ç¡®å®šæ€§
        if hasattr(self, 'uncertainty_results'):
            pred_mean = np.exp(self.uncertainty_results['pred_mean'])
            pred_lower = np.exp(self.uncertainty_results['pred_lower'])
            pred_upper = np.exp(self.uncertainty_results['pred_upper'])
            
            axes[1, 1].plot(years, actual_carbon, 'b-', linewidth=2, label='å®é™…å€¼')
            axes[1, 1].plot(years, pred_mean, 'r-', linewidth=2, label='é¢„æµ‹å‡å€¼')
            axes[1, 1].fill_between(years, pred_lower, pred_upper, alpha=0.3, color='red', label='95%ç½®ä¿¡åŒºé—´')
            axes[1, 1].set_xlabel('å¹´ä»½')
            axes[1, 1].set_ylabel('ç¢³æ’æ”¾é‡ (ä¸‡å¨CO2)')
            axes[1, 1].set_title('é¢„æµ‹ä¸ç¡®å®šæ€§')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        
        # 6. æ®‹å·®åˆ†å¸ƒ
        axes[1, 2].hist(residuals, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
        axes[1, 2].axvline(residuals.mean(), color='red', linestyle='--', label=f'å‡å€¼: {residuals.mean():.3f}')
        axes[1, 2].set_xlabel('æ®‹å·®')
        axes[1, 2].set_ylabel('é¢‘æ•°')
        axes[1, 2].set_title('æ®‹å·®åˆ†å¸ƒ')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {save_path}")
        
        return fig

def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºè´å¶æ–¯STIRPATå»ºæ¨¡ç³»ç»Ÿ"""
    print("ğŸš€ å¼€å§‹é˜¶æ®µäºŒï¼šå»ºæ¨¡æ–¹æ³•ä¼˜åŒ– - è´å¶æ–¯STIRPATå»ºæ¨¡")
    print("=" * 80)
    
    # 1. åˆå§‹åŒ–æ¨¡å‹
    model = BayesianSTIRPATModel(random_state=42)
    
    # 2. åŠ è½½æ•°æ®
    data = model.load_data()
    
    # 3. å‡†å¤‡å»ºæ¨¡æ•°æ®
    X_scaled, y = model.prepare_stirpat_data()
    
    # 4. æ‹Ÿåˆè´å¶æ–¯æ¨¡å‹
    model_results = model.fit_bayesian_model()
    
    # 5. ä¸ç¡®å®šæ€§é‡åŒ–
    uncertainty_results = model.uncertainty_quantification(n_samples=1000)
    
    # 6. äº¤å‰éªŒè¯
    cv_scores = model.cross_validation(cv_folds=5)
    
    # 7. ä¸ä¼ ç»Ÿæ–¹æ³•æ¯”è¾ƒ
    comparison_results = model.compare_with_traditional_methods()
    
    # 8. æƒ…æ™¯é¢„æµ‹
    scenarios = {
        'baseline_2030': {
            'population': 250,  # ä¸‡äºº
            'gdp': 8000,       # äº¿å…ƒ
            'technology': 150,  # æŠ€æœ¯æŒ‡æ•°
            'tourism': 5000    # ä¸‡äººæ¬¡
        },
        'low_carbon_2030': {
            'population': 240,  # ä¸‡äºº
            'gdp': 7500,       # äº¿å…ƒ
            'technology': 200,  # æŠ€æœ¯æŒ‡æ•°ï¼ˆæ›´é«˜ï¼‰
            'tourism': 4000    # ä¸‡äººæ¬¡ï¼ˆé€‚åº¦æ§åˆ¶ï¼‰
        }
    }
    
    predictions = model.scenario_prediction(scenarios)
    
    # 9. å¯è§†åŒ–ç»“æœ
    fig = model.visualize_results('/workspace/ä¼˜åŒ–å®æ–½/é˜¶æ®µäºŒ_å»ºæ¨¡æ–¹æ³•ä¼˜åŒ–/è´å¶æ–¯STIRPATå»ºæ¨¡ç»“æœ.png')
    
    # 10. æ€»ç»“æŠ¥å‘Š
    print(f"\nğŸ¯ é˜¶æ®µäºŒä¼˜åŒ–æ•ˆæœæ€»ç»“:")
    print("=" * 50)
    print("âœ… ä¸»è¦æ”¹è¿›:")
    print("â€¢ å¼•å…¥è´å¶æ–¯æ–¹æ³•ï¼Œæä¾›å‚æ•°ä¸ç¡®å®šæ€§é‡åŒ–")
    print("â€¢ èå…¥å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜å°æ ·æœ¬ä¸‹çš„ä¼°è®¡ç²¾åº¦")
    print("â€¢ æä¾›å®Œæ•´çš„é¢„æµ‹ç½®ä¿¡åŒºé—´")
    print("â€¢ ä¸ä¼ ç»Ÿæ–¹æ³•æ¯”è¾ƒï¼Œå±•ç¤ºä¼˜åŠ¿")
    
    print(f"\nğŸ“ˆ å…³é”®æŒ‡æ ‡æ”¹è¿›:")
    print(f"â€¢ æ¨¡å‹RÂ²: {model_results['r2_score']:.4f}")
    print(f"â€¢ äº¤å‰éªŒè¯ç¨³å®šæ€§: {cv_scores.std():.4f} (ç›®æ ‡: <0.1)")
    print(f"â€¢ ä¸ç¡®å®šæ€§é‡åŒ–: å®Œæ•´çš„95%ç½®ä¿¡åŒºé—´")
    print(f"â€¢ å‚æ•°è§£é‡Šæ€§: ç³»æ•° Â± æ ‡å‡†è¯¯")
    
    print(f"\nğŸ”® æƒ…æ™¯é¢„æµ‹ç»“æœ:")
    for scenario, pred in predictions.items():
        print(f"â€¢ {scenario}: {pred['carbon_pred']:.1f} ä¸‡å¨CO2 "
              f"[{pred['carbon_lower']:.1f}, {pred['carbon_upper']:.1f}]")
    
    print(f"\nğŸ‰ é˜¶æ®µäºŒï¼šå»ºæ¨¡æ–¹æ³•ä¼˜åŒ–å®Œæˆï¼")
    print("ä¸‹ä¸€æ­¥ï¼šè¿›å…¥é˜¶æ®µä¸‰ - ç®—æ³•æŠ€æœ¯ä¼˜åŒ–")

if __name__ == "__main__":
    main()